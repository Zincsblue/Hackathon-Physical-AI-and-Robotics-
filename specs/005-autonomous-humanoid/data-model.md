# Data Model: Autonomous Humanoid Capstone

**Feature**: 005-autonomous-humanoid
**Date**: 2025-12-19

## Overview
This document defines the key data structures and entities used in the Autonomous Humanoid Capstone project that integrates perception, language, planning, and control.

## Core Entities

### VoiceCommand
**Description**: Represents a spoken command from the user processed by the system
**Fields**:
- `id` (string): Unique identifier for the command
- `raw_audio` (bytes): Raw audio data captured from microphone
- `transcribed_text` (string): Text representation of the spoken command
- `confidence_score` (float): Confidence level of speech recognition (0.0-1.0)
- `timestamp` (datetime): Time when command was received
- `status` (enum): Processing status (RECEIVED, PROCESSING, PROCESSED, FAILED)

### TaskPlan
**Description**: Represents the sequence of actions generated by the LLM from a natural language command
**Fields**:
- `id` (string): Unique identifier for the plan
- `original_command` (string): The original voice command text
- `action_sequence` (array): Ordered list of actions to execute
- `reasoning` (string): Explanation of the planning decisions
- `estimated_duration` (float): Estimated time to complete the plan
- `confidence` (float): Confidence in the plan validity (0.0-1.0)
- `status` (enum): Plan status (PLANNED, EXECUTING, COMPLETED, FAILED)

### Action
**Description**: A single action within a task plan
**Fields**:
- `id` (string): Unique identifier for the action
- `action_type` (enum): Type of action (NAVIGATE, DETECT_OBJECT, MANIPULATE, SPEAK)
- `parameters` (object): Action-specific parameters (e.g., target location, object name)
- `description` (string): Human-readable description of the action
- `estimated_duration` (float): Estimated time to complete the action
- `status` (enum): Action status (PENDING, EXECUTING, COMPLETED, FAILED)

### NavigationGoal
**Description**: Represents a target location for the robot to navigate to in the simulation environment
**Fields**:
- `id` (string): Unique identifier for the navigation goal
- `position_x` (float): X coordinate in the simulation world
- `position_y` (float): Y coordinate in the simulation world
- `position_z` (float): Z coordinate in the simulation world
- `orientation_x` (float): X component of orientation quaternion
- `orientation_y` (float): Y component of orientation quaternion
- `orientation_z` (float): Z component of orientation quaternion
- `orientation_w` (float): W component of orientation quaternion
- `frame_id` (string): Coordinate frame reference
- `status` (enum): Navigation status (PENDING, IN_PROGRESS, REACHED, FAILED)

### PerceptionResult
**Description**: Represents the output of vision-based perception
**Fields**:
- `id` (string): Unique identifier for the perception result
- `timestamp` (datetime): Time when perception was performed
- `detected_objects` (array): List of objects detected in the scene
- `scene_description` (string): Textual description of the scene
- `confidence` (float): Overall confidence in perception results (0.0-1.0)
- `image_data` (bytes): Raw image data processed

### DetectedObject
**Description**: Represents a single object detected in the environment
**Fields**:
- `id` (string): Unique identifier for the detected object
- `name` (string): Object name or classification
- `position_x` (float): X coordinate of object in world frame
- `position_y` (float): Y coordinate of object in world frame
- `position_z` (float): Z coordinate of object in world frame
- `confidence` (float): Confidence in object detection (0.0-1.0)
- `bounding_box` (object): 2D bounding box in image coordinates
- `color` (string): Dominant color of the object

### ActionResult
**Description**: Represents the outcome of executing a specific action
**Fields**:
- `id` (string): Unique identifier for the action result
- `action_id` (string): Reference to the action that was executed
- `success` (boolean): Whether the action was successful
- `execution_time` (float): Time taken to execute the action
- `result_data` (object): Action-specific result data
- `error_message` (string): Error message if action failed
- `confidence` (float): Confidence in the action outcome (0.0-1.0)

## Relationships

```
VoiceCommand (1) → (1) TaskPlan
TaskPlan (1) → (many) Action
NavigationGoal (1) → (many) ActionResult
PerceptionResult (1) → (many) DetectedObject
Action (1) → (1) ActionResult
```

## State Transitions

### VoiceCommand State Transitions
RECEIVED → PROCESSING → [PROCESSED | FAILED]

### TaskPlan State Transitions
PLANNED → EXECUTING → [COMPLETED | FAILED]

### Action State Transitions
PENDING → EXECUTING → [COMPLETED | FAILED]

### NavigationGoal State Transitions
PENDING → IN_PROGRESS → [REACHED | FAILED]

## Validation Rules

1. **VoiceCommand**:
   - `transcribed_text` must not be empty
   - `confidence_score` must be between 0.0 and 1.0
   - `timestamp` must be within the last 5 minutes

2. **TaskPlan**:
   - `action_sequence` must contain at least one action
   - `confidence` must be between 0.0 and 1.0
   - `estimated_duration` must be positive

3. **Action**:
   - `action_type` must be a valid enum value
   - `estimated_duration` must be positive

4. **NavigationGoal**:
   - Position coordinates must be within simulation bounds
   - `frame_id` must be a valid coordinate frame

5. **PerceptionResult**:
   - `timestamp` must be recent
   - `confidence` must be between 0.0 and 1.0

6. **DetectedObject**:
   - `confidence` must be between 0.0 and 1.0
   - Position coordinates must be within expected ranges

7. **ActionResult**:
   - `confidence` must be between 0.0 and 1.0 if provided